<!DOCTYPE html>
<html lang="" xml:lang="">
  <head>
    <title>Prior Choice</title>
    <meta charset="utf-8" />
    <meta name="author" content="Shangchen Song" />
    <link href="libs/anchor-sections/anchor-sections.css" rel="stylesheet" />
    <script src="libs/anchor-sections/anchor-sections.js"></script>
    <link rel="stylesheet" href="xaringan-themer.css" type="text/css" />
  </head>
  <body>
    <textarea id="source">
class: center, middle, inverse, title-slide

# Prior Choice
## how to choose <br/>a good prior
### Shangchen Song
### UF
### 2020/12/12 (updated: 2020-11-29)

---





# The Role of Prior Distribution

The prior distribution is a key part of Bayesian inference and represents the information about an uncertain parameter `\(\theta\)` that is combined with the probability distribution of new data to yield the **posterior distribution**, which in turn is used for future inferences and decisions involving `\(\theta\)` .


&lt;br/&gt;
$$
\text{posterior} \propto  \text{likelihood} \times \text{prior}
$$

---

# Key Issues for Prior Choice


1. What information is going into the prior distribution.

  - Should be include all possible values of the parameter of inserst
&lt;br/&gt;&lt;br/&gt;
2. The properties of the resulting posterior distribution.

  - Proper or improper;
  - Closed form or not, although can be overcome by MCMC algorithm.


---

# The Influence of Prior Distribution

- With large sample sizes, reasonable choices of prior distributions will have minor effects on posterior inferences. In practice one can check the dependence on prior distributions by a sensitivity analysis: comparing posterior inferences under different reasonable choices of prior distribution

- If the sample size is small, the prior distribution becomes more important. In many cases, however, models can be set up hierarchically, so that clusters of parameters have shared prior distributions, which can themselves be
estimated from previous data.

---

# Example: pharmacokinetics

We illustrate with an example from a model in pharmacokinetics, the study of the absorption, distribution, and elimination of drugs from the body. 

For this particular study, 

- about 20 measurements were available on six young adult males, 

- and a model was fit with 15 parameters per person (which we label `\(\theta_{kl}\)` for person `\(k\)` and parameter `\(l\)`), 

- along with two variance parameters, `\(\sigma_1^2\)` and `\(\sigma^2_2\)`, indicating the scale of measurement/modeling error. 

The data (concentrations of a compound in blood and exhaled air over time) are only indirectly informative of the individual level parameters, which refer to equilibrium concentrations, volumes, and metabolic rates inside the body.

This is a nice example to use here because different principles for assigning prior distributions are relevant for different parameters in the model, as we
now discuss.

---

# Noninformative Prior Distributions

We first consider the variance parameters `\(\sigma_1^2\)` and `\(\sigma^2_2\)`, which are actually quite well identified in the posterior distribution. For these, a noninformative uniform prior distribution works fine. (A uniform distribution on the log standard deviations was used, but enough information was available from the data that the choice of noninformative prior distribution was essentially irrelevant, and one could just as well have assigned a uniform prior distribution on the variances or the standard deviations.)

The uniform prior distribution here is improper – that is, the function used
as a ‘prior probability density’ has an infinite integral
and is thus not, strictly speaking, a probability density at all. However, when formally combined with
the data likelihood it yields an acceptable proper posterior distribution.

---

# Highly Informative Prior Distributions

At the other extreme, fairly precise scientific information is available on some of the parameters `\(\theta_{kl}\)` in the model. 

For example, parameter 8 represents the mass of the liver as a fraction of lean body mass; from previous medical studies, the liver is known to be about 3.3% of lean body mass for young adult males, with little variation.



The prior distribution for `\(\log \theta_{k,8}\)` (for persons k = 1,..., 6) is assumed normal with mean `\(\mu_8\)` and standard deviation `\(\Sigma_8\)`; `\(\mu_8\)` was given a normal prior distribution with mean `\(\log(0.033)\)` and standard deviation `\(\log1.1\)`, and `\(\Sigma_8\)` was given an inverse `\(\chi^2\)` prior distribution with scale `\(\log(1.1)\)` and two degrees of freedom. This setup sets the parameters `\(\theta_{k,8}\)` approximately to their prior estimate, 0.033, with some variation allowed between persons.

---
## Moderately Informative Hierarchical Prior Distributions

Finally, some of the physiological parameters `\(\theta_{kl}\)` are not well estimated by the data – thus, they require informative prior distributions – but scientific information on them is limited.

For example, in this particular study, parameter 14 represents the maximum rate of metabolism of a certain compound; the best available estimate of this parameter for healthy humans is 0.042, but this estimate is quite crude
and could easily be off by a factor of 10 or 100. The maximum rate of metabolism is not expected to vary greatly between persons, but there is much uncertainty about the numerical value of the parameter. This information is encoded in a *hierarchical prior distribution*: `\(\log \theta_{k, 14} \sim N(\mu_{14}, \Sigma_{14})\)` with `\(\mu_{14} \sim N(\log(0.042), \log^2(10))\)` and `\(\Sigma_{14} \sim Inv \chi^2 (2, \log^2(2))\)`


---

### What Would Happen if Noninformative Prior Distributions Were Used for All the Parameters in this Example?

In our parameterization, noninformative prior distributions on the parameters `\(\theta_{kl}\)` correspond to setting `\(\Sigma_{.l} = \infty\)` for each parameter `\(l\)`, thus allowing each person’s parameters to be estimated from that person’s own data. 

If noninformative prior distributions were assigned to all the individual parameters, then the model would fit the data very closely but with
scientifically unreasonable parameters – for example, a person with a liver weighing as value `\(10\)`. This sort of difficulty is what motivates a researcher to specify a prior distribution using external information.

---

# Theory

Most of the theoretical work on prior distributions has been on two topics:

- first, determining the conditions that must be satisfied by the prior and data distributions so that the posterior distribution is well behaved; 

- second, setting up rules for noninformative prior distributions that satisfy various invariance principles.

These strands of research are related, in that prior distributions set up based on invariance rules alone will make sense only if they lead to reasonable posterior distributions.


---
# Theory

Perhaps the most well known theoretical result is that, for variance parameters in a linear regression model, the uniform prior distribution for `\(\log(\sigma)\)` is acceptable when applied to the lowest level variance component but not acceptable for higher level variance components. i.e. `\(\log \sigma \propto 1\)` and `\(\tau \propto 1\)` for proper posterior.

These theoretical results do not give **recommended** models, but rather are useful in **ruling out** certain natural seeming models with poor statistical properties.


---
# Summary

Conversely, one might seek to avoid theoretical considerations entirely and simply pick a ‘subjective’ prior distribution that best represents one’s scientific knowledge about the set of uncertain parameters in the problem. 

In practice, however, subjective knowledge is hard to specify precisely, and so it is important to study the sensitivity of posterior inferences. In many problems, the key issue in setting up the prior distribution is the specification of the model into parameters that can be clustered hierarchically.

---
# Priors can be created using a number of methods.

- A prior can be determined from past information, such as previous experiments.

- A prior can be elicited from the purely subjective assessment of an experienced expert. 

- An uninformative prior can be created to reflect a balance among outcomes when no information is available. 

- Priors can also be chosen according to some principle, such as symmetry or maximizing entropy given constraints; examples are the Jeffreys prior or Bernardo's reference prior. 

- When a family of conjugate priors exists, choosing a prior from that family simplifies calculation of the posterior distribution.

---
# Five Levels of Priors

--

- Flat prior (not usually recommended);

--

- Super-vague but proper prior: `\(N(0, 1e6)\)` (not usually recommended);

--

- Weakly informative prior, very weak: `\(N(0, 10)\)`;

--

- Generic weakly informative prior: `\(N(0, 1)\)`;

--

- Specific informative prior: `\(N(0.4, 0.2)\)` or whatever. Sometimes this can be expressed as a scaling followed by a generic prior: `\(\theta = 0.4 + 0.2*z\)`; `\(z \sim N(0, 1)\)`;

---

# Parameter of Prior Distribution

Parameters of prior distributions are a kind of **hyperparameter**. For example, if one uses a beta distribution to model the distribution of the parameter `\(p\)` of a Bernoulli distribution, then:

- `\(p\)` is a parameter of the underlying system (Bernoulli distribution), and
- `\(\alpha\)` and `\(\beta\)` are parameters of the prior distribution (beta distribution); hence hyperparameters.

Hyperparameters themselves may have hyperprior distributions expressing beliefs about their values. 

A Bayesian model with more than one level of prior like this is called a hierarchical Bayes model.


---

# Informative priors

#### An **informative prior** expresses specific, definite information about a variable. 

An example is a prior distribution for the temperature at noon tomorrow. 

A reasonable approach is to make the prior a normal distribution with expected value equal to today's noontime temperature, with variance equal to the day-to-day variance of atmospheric temperature, or a distribution of the temperature for that day of the year.
---

# Weakly informative priors

A weakly informative prior expresses partial information about a variable. An example is, when setting the prior distribution for the temperature at noon tomorrow in St. Louis, to use a normal distribution with mean 50 degrees Fahrenheit and standard deviation 40 degrees, which very loosely constrains the temperature to the range (10 degrees, 90 degrees) with a small chance of being below -30 degrees or above 130 degrees. The purpose of a weakly informative prior is for regularization, that is, to keep inferences in a reasonable range.

---

# Uninformative priors

#### An uninformative prior or diffuse prior expresses vague or general information about a variable. The term "uninformative prior" is somewhat of a misnomer. Such a prior might also be called a not very informative prior, or an objective prior, i.e. one that's not subjectively elicited.

Uninformative priors can express "objective" information such as "the variable is positive" or "the variable is less than some limit". The simplest and oldest rule for determining a non-informative prior is the principle of indifference, which assigns equal probabilities to all possibilities. In parameter estimation problems, the use of an uninformative prior typically yields results which are not too different from conventional statistical analysis, as the likelihood function often yields more information than the uninformative prior.

---
# Reference
- [Prior distribution Andrew Gelman](http://www.stat.columbia.edu/~gelman/research/published/p039-_o.pdf)




---

class: inverse, center, middle

# Thanks!
    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"highlightStyle": "github",
"highlightLines": true,
"countIncrementalSlides": false
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();
(function() {
  "use strict"
  // Replace <script> tags in slides area to make them executable
  var scripts = document.querySelectorAll(
    '.remark-slides-area .remark-slide-container script'
  );
  if (!scripts.length) return;
  for (var i = 0; i < scripts.length; i++) {
    var s = document.createElement('script');
    var code = document.createTextNode(scripts[i].textContent);
    s.appendChild(code);
    var scriptAttrs = scripts[i].attributes;
    for (var j = 0; j < scriptAttrs.length; j++) {
      s.setAttribute(scriptAttrs[j].name, scriptAttrs[j].value);
    }
    scripts[i].parentElement.replaceChild(s, scripts[i]);
  }
})();
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
// adds .remark-code-has-line-highlighted class to <pre> parent elements
// of code chunks containing highlighted lines with class .remark-code-line-highlighted
(function(d) {
  const hlines = d.querySelectorAll('.remark-code-line-highlighted');
  const preParents = [];
  const findPreParent = function(line, p = 0) {
    if (p > 1) return null; // traverse up no further than grandparent
    const el = line.parentElement;
    return el.tagName === "PRE" ? el : findPreParent(el, ++p);
  };

  for (let line of hlines) {
    let pre = findPreParent(line);
    if (pre && !preParents.includes(pre)) preParents.push(pre);
  }
  preParents.forEach(p => p.classList.add("remark-code-has-line-highlighted"));
})(document);</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
